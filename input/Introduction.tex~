\textbf{Compiler:}\\\\
A compiler is a software program which converts the code written in any programming language i.e high-level language into a low-level language code.A compiler is a translator whose source language is a high-level language and whose object language is close to the machine language of an actual computer.The typical compiler consists of several phases each of which passes its output to the next phase\\\\
\textbf{Phases of Compiler:}
\begin{itemize}
\item Lexical analysis: The lexical phase (scanner) groups characters into lexical units or tokens. The input to the lexical phase is a character stream. The output is a stream of tokens. Regular expressions are used to define the tokens recognized by a scanner (or lexical analyzer). The scanner is implemented as a finite state machine. Lex and Flex are tools for generating scanners: programs which recognize lexical patterns in text. Flex is a faster version of Lex.
\item Syntax analysis: The parser groups tokens into syntactical units. The output of the parser is a parse tree representation of the program. Context-free grammars are used to define the program structure recognized by a parser. The parser is implemented as a push-down automata.
\item Semantic analysis: The semantic analysis phase analyzes the parse tree for context-sensitive information often called the static semantics. The output of the semantic analysis phase is an annotated parse tree. Attribute grammars are used to describe the static semantics of a program.
\item Intermediate Code Generation: This phase is often combined with the parser. During the parse, information concerning variables and other objects is stored in a symbol table. The information is utilized to perform the context-sensitive checking.
\item Code optimization: The optimizer applies semantics preserving transformations to the annotated parse tree to simplify the structure of the tree and to facilitate the generation of more efficient code.
\item Code generation: The code generator transforms the simplified annotated parse tree into object code using rules which denote the semantics of the source language. The code generator may be integrated with the parser.
\end{itemize}

\textbf{Parser:}\\\\
Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, according to the rules of a formal grammar.A parser is a program which determines if its input is syntactically valid and determines its structure. Parsers may be hand written or may be automatically generated by a parser generator from descriptions of valid syntactical structures. The descriptions are in the form of a context-free grammar.\\\\
Yacc is a program which given a context-free grammar, constructs a C program which will parse input according to the grammar rules.\\\\

\begin{figure} [h]
\centering
\includegraphics[width=0.5\textwidth]{images/flexdev.jpeg}
\caption{Donald Knuth}
\end{figure}

Parser invented by Donald Knuth in 1965. Parser breaks data into smaller elements for easy translation into another language. A parser takes input in the form of a sequence of tokens and usually builds a data structure in the form of a parse tree or an abstract syntax tree.\\\\

\textbf{Flex:}
The fast lexical analyser. It is the modern replacement for the classic Lex, which was developed by the Bell Laboratories in the 1970s.
Flex was originally written by Jef Poskanzer; Vern Paxson and Van Jacobson have considerably improved it. Flex is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. The flex program reads user-specified input files, for a description of a scanner to generate. The description is in the form of pairs of regular expressions and C code, called rules. Flex generates a C source file named, "lex.yy.c", which defines the function yylex(). The file "lex.yy.c" can be compiled and linked to produce an executable. When the executable is run, it analyzes its input for occurrences of text matching the regular expressions for each rule. Whenever it finds a match, it executes the corresponding C code.Flex and Bison files have three sections:\\\\
\begin{itemize}
\item The first is sort of "control" information
\item The second is the actual token/grammar definitions
\item The last is C/C++ code to be copied verbatim to the output
\end{itemize}

\textbf{Bison:}
Yacc and Bison are tools for generating parsers: programs which recognize the structure grammatical structure of programs. Bison is a faster version of Yacc. Bison was originally written by Robert Corbett in 1988.The sections on Yacc/Bison are a condensation and extension of the document “BISON the Yacc-compatible Parser Generator” by Charles Donnelly and Richard Stallman. Like Flex, Bison file is also divided into three sections divided by \%\%. Bison reads a specification of the user-specified grammar, warns about any parsing ambiguities, and generates a parser (either in C, C++, or Java) which reads sequences of tokens and decides whether the sequence conforms to the syntax specified by the grammar.A input file for Bison is of the form:\\

\begin{itemize}
\item C and parser declarations
\item \%\%
\item Grammar rules and actions
\item \%\%
\item C subroutines
\end{itemize}



Various tools used to develop the project are:\\
\begin{itemize}
\item Flex/lex
\item Bison/yacc
\item C++
\item C language
\end{itemize}

\subsection{Software requirements}
\begin{itemize}
\item Operating System: Linux/Windows
\item Programming Language: C, C++
\end{itemize}

\subsection{Hardware Requirements}
Hardware requirement of this project is any Desktop or Laptop machine for local use or a Server with minimum available configuration to make Project globally available. Hardware specifications of the machine used depends upon the hardware requirements of the Operating System installed on it. As such there are no special hardware requirements of this project.\\
